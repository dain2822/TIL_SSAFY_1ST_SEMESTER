## 251128

### 문서 유사도

: 여러 개의 문서(또는 문장) 중에서 “얼마나 비슷한 내용을 담고 있는지”를 수치로 표현한 것

- 추천 시스템
    
    : 도서, 영화, 금융 상품 등에서 사용자의 관심사와 유사한 콘텐츠를 찾아 추천
    
- 검색 및 정보 탐색
    
    : 입력 쿼리(질문)와 유사한 문서를 빠르게 찾고, 검색 결과 품질을 높임
    
- 분류, 군집 및 중복 검출
    
    : 뉴스 기사, 논문 등이 서로 얼마나 유사한지 계산 → 중복 기사 묶음 처리, 토픽별 문서 분류
    
- 의사결정 지원
    
    : 방대한 문서(보고서, 계약서 등) 중 유사 문서나 핵심 정보를 신속히 파악 가능
    

: 텍스트(문자)로 이루어진 문서라면, 사람이 읽고 비교하기 쉬움

: 아래 예시 중, 가장 “유사”한 문장 2개를 선택한다면?

1. 먹고 싶은 사과
2. 먹고 싶은 바나나
3. 길고 노란 바나나 바나나
4. 저는 과일이 좋아요

→ 1번, 2번 문장이 “가장 유사”하다고 선택할 것

→ 그 외 문장들 간의 관계에 대해서도 “어느 정도 연관은 있음”이라고 판별

→ 문장의 길이가 아주 길어진다면 어떻게 판별할까?

→ 비교해야 하는 문서의 양이 많아진다면?

→ 정확히 어느 정도로 유사한 지를 정량화 하고자 한다면?

### 문서 유사도 계산 방법

**코사인 유사도**

: 두 벡터가 이루는 각도의 코사인 값을 이용

: -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단

**자카드 유사도**

: 두 집합의 교집합 크기를 합집합 크기로 나눈 값

: 텍스트 유사도 측정 시 단어 존재 여부만 고려

**유클리드 거리**

: 공간적 거리 측정 방식으로, n차원 공간에서 두 점 사이의 직선 거리를 계산

: 거리가 0에 가까울수록 유사도가 높다고 판단

**벡터**

: 여러 수치를 일렬로 나열

: “방향과 크기”를 나타내는 수학적 표현

: 원점 (0, 0)에서 좌표 (3, 4)에 대한 벡터 표현

![image.png](attachment:7d75fbd4-4e9a-47b3-8f84-9465aa723c3f:image.png)

![image.png](attachment:39e42005-9182-4cea-8184-8cac132570dd:image.png)

**코사인**

: 빗변에 대한 인접변의 비

: $x축의 길이 = cosθ$

![image.png](attachment:3a0b68cd-6b45-46b3-9224-5e504be3db6a:image.png)

![image.png](attachment:97431fdc-55f3-4a92-b348-c31d62fa658c:image.png)

### 코사인 유사도

: 두 벡터가 이루는 각도의 코사인 값을 이용

: -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단

![image.png](attachment:d676d019-011c-48e0-b70a-2694598a1d83:image.png)

![image.png](attachment:3daed4c9-081f-4841-8d0d-d2951c1cf4ab:image.png)

- 두 벡터의 내적 계산
    
    : 벡터 A와 벡터 B의 내적은 동일 차원끼리 곱한 후, 모두 더한 값
    
     $A * B = (Ax * Bx) + (Ay * By) = (3 * 4) + (3 * 1) = 15$
    
- 두 벡터의 크기(노름) 계산
    
    : 벡터의 크기는 피타고라스 정리를 이용
    
    : 각 좌표의 제곱의 합에 루트를 씌운 것
    
    ![image.png](attachment:42a88e5e-3670-4da5-b623-96e8e808f0f3:image.png)
    
- 코사인 유사도
    
    ![image.png](attachment:45b8850f-e7b8-4427-8b42-70c70f1fdff6:image.png)
    
    ![image.png](attachment:9dc8d5c3-82de-4f83-8bca-fb54a1fe1491:image.png)
    
    ![image.png](attachment:1d8ab9ea-c0db-4aaa-8d2c-187b74384984:image.png)
    

→ 두 벡터의 관계를 코사인 값으로 나타내는 것이 목표

→ 벡터를 활용하므로 2차원 데이터가 아닌 경우에도 연산에는 문제 없음

→ n차원으로 표기되는 데이터가 아닌 경우에도 연산에는 문제 없음

→ ex) 다음과 같은 경우에도 코사인 유사도식 연산 가능

- $vector A = (x1, x2, …, xn)$
- $vector B = (y1, y2, …, yn)$

### 카운트 기반 문서 유사도

: 코사인 유사도 방식으로 두 벡터 간의 유사도를 알아내려면 벡터 값이 필요

: 주어진 데이터는 텍스트(문자) 형태의 데이터

: 텍스트 기반 데이터를 벡터 데이터로 변환할 수 있는 방법이 필요

**Bag of Words**

- 고유 단어 추출
    
    : 불용어(조사, 접속사 등)를 제거 → 더 높은 정확도
    
- 각 단어의 인덱스 부여
    
    : 사전순 혹은 등장 순서대로 인덱스를 부여
    
- 문서별 각 단어의 등장 빈도수 기록
    
    : 문서 내 등장 횟수를 해당 단어의 인덱스 위치에 기록
    
- 문서별 빈도 벡터 정보
    
    ![image.png](attachment:01e45392-5337-4984-af12-38477b703f4c:image.png)
    
    → 불용어 제거 과정을 거치지 않음
    

**Bag of Words 구현**

- 문서 정의
- 단어 카운트 행렬 생성

![image.png](attachment:dae7a5ca-b8e5-43e8-9b65-3e3d5e6d86a7:image.png)

![image.png](attachment:30941a64-4178-4cc5-b60e-d099ba715bb0:image.png)

- numpy 배열로 변환

![image.png](attachment:dbf464c6-6039-43d7-8513-1159ebe2868f:image.png)

![image.png](attachment:30eb74b0-3fff-4906-8179-e05b1a1e5ad8:image.png)

- 문서간 코사인 유사도 계산

![image.png](attachment:6f4ba32b-8de5-4dc2-a4ef-9562e2d1c05f:image.png)

**카운트 기반 문서 유사도 한계점**

- 문서(단어) 벡터 간 유의미한 유사도를 계산할 수 없음
- ‘사과’와 ‘바나나’는 과일이라는 공통점을 가지고 있으나, 해당 정보를 단어 카운트 행렬에서는 반영할 수 있는 방법이 없음
    
    → 그 결과 1번 문서와 3번 문서의 유사도가 0으로 나오게 됨
    

### 임베딩 기반 문서 유사도

**워드 임베딩_ Word Embedding**

: 텍스트를 ‘의미’를 반영한 숫자 벡터로 변환하는 기법

: 단순히 단어 등장 횟수를 세는 방식(Bag of Words 등)으로는 ‘사과’와 ‘배’가 과일이라는 의미적 유사도를 충분히 반영하기 어려움

: 임베딩 과정을 통해, 서로 의미가 비슷한 단어, 문장이 고차원 벡터 공간 상에서 가까이 위치하도록 학습시킴

: 임베딩 된 벡터들은 서로 코사인 유사도 등으로 간단히 비교 가능하며, 훨씬 정교한 문서(단어)간 계산이 가능해짐

- 데이터 수집
    
    : 실습에서는 ‘대한민국 헌법.txt’ 파일 사용
    
- 데이터 전처리
    - 토큰화 : 띄어쓰기(또는 형태소) 기준으로 분리
    - 한글이 아닌 표현(이모지, 꺽쇠 등) 제거
    - 불용어 제거 : 벡터로 만들 필요가 없거나 지나치게 많이 사용되는 단어 제거(조사 등)
- 임베딩용 딥러닝 모델 설계(Word2Vec, Doc2Vec 등 사용 예정)

**Word2Vec**

- 분포 가설에 기반하여 단어의 의미를 벡터로 ‘학습’
    
    → “서로 비슷한 맥락에서 등장하는 단어들은 의미도 비슷함”
    
- 주변 단어를 보고 중심 단어를 맞히거나 학습하면, 단어 간 의미 관계가 유사한 벡터로 학습
- 학습 방식
    - CBOW : 주변 단어들을 입력으로 받고(맥락), 그 중심 단어를 예측하는 방식
    - Skip-gram : 중심 단어를 입력으로 받고, 주변 단어들을 예측하는 방식
- CBOW 예시
    
    ![image.png](attachment:c491697b-7e83-481d-88e9-0f7efa38b18b:image.png)
    
    ![image.png](attachment:efc14be7-4eef-4f7e-99b1-4cebd62d8baf:image.png)
    

**Word2Vec 활용**

- 필요한 라이브러리 설치
    - Gensim
        
        : 자연어 처리 라이브러리
        
        : Word2Vec, Doc2Vec 등 임베딩 관련 알고리즘 지원
        
    - Konlpy
        
        : 한국어 자연어 처리(NLP)에 특화된 라이브러리
        
        : 형태소 분석기, 불용어 제거 등의 기초 전처리 지원
        

![image.png](attachment:7ec5568d-53f3-42ce-8bfd-ec9a10c2d3ae:image.png)

- 필요한 라이브러리 import

![image.png](attachment:f4347af6-d2ac-4c68-bbe0-d9cb165f3f17:image.png)

- 데이터 수집
    - ‘대한민국 헌법.txt’ 파일 업로드
    
    ![image.png](attachment:0f133301-9455-47f6-ac8b-36358a3fe1cd:image.png)
    
- 데이터 전처리 - 불필요한 문자 제거
    
    ![image.png](attachment:72564c51-6aa3-4656-a243-fbaca4004b4b:image.png)
    
- 데이터 전처리 - 문장을 단어로 분리(토큰화)
    
    ![image.png](attachment:e73d17bd-4037-4757-bac6-dc607dd61acc:image.png)
    
- 한글 토큰을 100차원의 워드 임베딩 벡터로 변환
    
    ![image.png](attachment:595fe89d-6e40-4ba9-b8a4-f4e0c8910025:image.png)
    
- 출력 결과 확인
    
    ![image.png](attachment:d158f67e-42b7-43a6-b70e-ff3ed288d4f9:image.png)
    
    - 불용어 처리 시 상황에 따라 적절한 처리가 필요한 이유
        - 언어별 특성을 잘 반영해야 함
        - 문서/도메인의 특성이 잘 반영되어야 함
            
            → 예를 들어, 법률 문서에서는 ‘제, 조, 항’ 등이 불용어일 수 있지만, 다른 문서에서는 의미가 다를 수 있음
            
- 불용어 정의 업데이트 및 코드 재실행 후 결과 확인
    
    ![image.png](attachment:80696603-0c16-476e-bb55-e51f04526503:image.png)
    
- ‘법률’ 단어의 100차원 벡터 임베딩 결과 확인
    
    ![image.png](attachment:181edab6-79b4-4ea0-ab6d-3ef8076618cd:image.png)
    
- -1 부터 1 사이로 이루어진 좌표값
    
    ![image.png](attachment:c2f33397-0397-488b-9323-70b6e192eee0:image.png)
    
- 총 길이 : 100
- 코사인 유사도 함수
    
    → ‘법률’ 단어와 ‘헌법’ 단어 벡터 간의 코사인 유사도 계산
    
    ![image.png](attachment:8a21168b-5a29-42c0-afa0-1ae01c4274a7:image.png)
    
- 특정 단어와 가장 유사한 단어 출력

![image.png](attachment:a988fb41-eea9-4b75-8684-9d077aa0410e:image.png)

- 헌법과 유사한 단어 Top 10

![image.png](attachment:2b44222a-757c-44ec-8a45-904b402c353d:image.png)

**Doc2Vec**

- 도큐먼트 임베딩
    
    : 여러 단어로 이루어진 문장, 문서를 임베딩하는 방법
    
    : 문서별로 고유 문서 태그(문서 ID)를 부여
    
    : Word2Vec과 유사하게 중심 단어와 주변 단어를 예측
    
    - 문서(문장)를 대표하는 문서 태그를 은닉층에 함께 학습
    - 이 문서 태그 임베딩이 결국 해당 문서를 대표하는 벡터가 됨
    
    : 문서의 단어들을 입력으로 받고(또는 Skip-Gram이면 단어를 예측) Doc2Vec 모델을 사용하여 “문서 태그 + 단어 임베딩”을 동시에 업데이트
    

**Doc2Vec 활용**

- 필요한 라이브러리 설치
    - Gensim 설치 후, 런타임 > ‘세션 다시 시작’ 실행
- Doc2Vec 및 Okt 불러오기
    
    ![image.png](attachment:95de7b28-b20b-4b21-a472-49541a21763a:image.png)
    
- 한국어 문장을 임베딩 벡터로 변환
    
    ![image.png](attachment:878e8f78-e218-416d-8548-55b4a65562c4:image.png)
    
- 한글 문장을 300 차원 벡터로 변환하기 위한 Doc2Vec 모델 생성
- alpha(학습률) : 모델이 학습을 통해 가중치를 업데이트하는 정도
    
    → 학습률이 높을 수록 가중치 업데이트가 크게 이루어짐
    
    ![image.png](attachment:84c175dd-2bbc-4c57-915b-1df956e3b23d:image.png)
    
- 모델이 학습할 문서 빌드 및 Doc2Vec 학습
    
    ![image.png](attachment:0de33043-c5f1-4e0e-ac70-ccaa51223ff5:image.png)
    
- 첫 번째 문장과 문장별 유사도 출력
    
    ![image.png](attachment:ab7aa776-6b20-4e15-a567-0a502d28c925:image.png)
    
- 결과가 만족스럽지 못한 이유?
    1. 문장 길이가 너무 짧고 단어 수가 적음
        
        : 학습 정보가 부족하여, 모델이 제대로 구분할 만한 특징을 잡기 어려움
        
    2. 주제별 키워드가 불충분하거나 겹치는 어휘가 많음
        
        : “기계학습”, “데이터” 등 핵심 단어가 충분히 반복되어야 함
        
    3. 학습 파라미터 및 데이터 규모 한계 
        
        : 소수의 짧은 문장만으로 Doc2Vec을 학습하면, 토픽별 임베딩이 명확히 분리되지 않을 수 있음
        

![image.png](attachment:0bdeeed9-4513-491d-a1ff-ed285429dc92:image.png)
